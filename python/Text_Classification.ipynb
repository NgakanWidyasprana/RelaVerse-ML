{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dmzvod0pX61b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeXATNXBBPVp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from IPython.core.display import display, HTML\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "# Natural Language Tool Kit\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from collections import Counter\n",
        "import cufflinks as cf\n",
        "cf.go_offline()"
      ],
      "metadata": {
        "id": "5FrjS8UIWlGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/Train.csv\")\n",
        "test_df = pd.read_csv(\"https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/Test.csv\")"
      ],
      "metadata": {
        "id": "7hUHzxt9BZpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "id": "3uhziBX3AyZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "id": "gqIJu-bEBLUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "\n",
        "id_stopword_dict = pd.read_csv('https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/stopwordbahasa.csv', header=None)\n",
        "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})"
      ],
      "metadata": {
        "id": "zAGpkhUPadhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(id_stopword_dict)"
      ],
      "metadata": {
        "id": "JwYGJo06BhNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopword(text):\n",
        "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "LLtK_YNsc8ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    # split to array (default delimiter is \" \")\n",
        "    text = remove_stopword(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "aEhyO2vXXzlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : clean_text(x))\n",
        "test_df['Title Campaign'] = test_df['Title Campaign'].apply(lambda x : clean_text(x))"
      ],
      "metadata": {
        "id": "Qa2Ip9gpdZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "9pJnryuvd1jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "qzWhbdQkeG0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySastrawi"
      ],
      "metadata": {
        "id": "l2PIM787eQ9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steaming\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def steaming(text):\n",
        "  return stemmer.stem(text)"
      ],
      "metadata": {
        "id": "1PX9_yFBewYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer)"
      ],
      "metadata": {
        "id": "sTsdiL8RCq9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : steaming(x))\n",
        "test_df['Title Campaign'] = test_df['Title Campaign'].apply(lambda x : steaming(x))"
      ],
      "metadata": {
        "id": "HZi9KsWKCras"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "neMeg5xCfPw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "bZ_5_IOcfVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many unique words have this text\n",
        "\n",
        "def counter_word(text):\n",
        "    count = Counter()\n",
        "    for i in text.values:\n",
        "        for word in i.split():\n",
        "            count[word] += 1\n",
        "    return count\n",
        "\n",
        "text_values = train_df['Title Campaign']\n",
        "\n",
        "counter = counter_word(text_values)\n",
        "print(f\"The len of words is: {len(counter)}\")\n",
        "list(counter.items())[:10]"
      ],
      "metadata": {
        "id": "tRJb-GkIfay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "\n",
        "vocab_size = len(counter)\n",
        "embedding_dim = 32\n",
        "\n",
        "# Max number of words in each complaint\n",
        "max_length = 20\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "\n",
        "# oov_took its set for words out our word index\n",
        "oov_tok = \"<XXX>\"\n",
        "training_size = 80\n",
        "seq_len = 12\n",
        "\n",
        "# based on 80% of the data\n",
        "training_sentences = train_df['Title Campaign'][0:training_size]\n",
        "training_labels = train_df.Label[0:training_size]\n",
        "\n",
        "valid_sentences = train_df['Title Campaign'][training_size:]\n",
        "valid_labels = train_df.Label[training_size:]\n",
        "\n",
        "print('The Shape of training ',training_sentences.shape)\n",
        "print('The Shape of validation',valid_sentences.shape)"
      ],
      "metadata": {
        "id": "tP1GSGeYfs_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "e9MqP4QgiMFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "id": "ExZy8u2sDdPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the first 10 elements\n",
        "print(\"THe first word Index are: \")\n",
        "for x in list(word_index)[0:15]:\n",
        "    print (\" {},  {} \".format(x,  word_index[x]))"
      ],
      "metadata": {
        "id": "ocixmswhioHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "XNfCESC-iq_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence : {}\".format(train_df['Title Campaign'][0]))\n",
        "print(\"Text Sequences : {}\".format(training_sequences[0]))\n",
        "print(\"Text Padded : {}\".format(training_padded[0]))"
      ],
      "metadata": {
        "id": "Z0uBG_GgET3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
        "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "CA4_wyBgj566"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence : \\n{}\".format(pd.Series(data=valid_sentences, index = [80])))\n",
        "print(\"\\nText Sequences : {}\".format(valid_sequences[0]))\n",
        "print(\"Text Padded : {}\".format(valid_padded[0]))"
      ],
      "metadata": {
        "id": "G2ExJ9ByEGJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # remember this is a binary classification\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H6hfACKEkIAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_time = time.time()\n",
        "\n",
        "num_epochs = 40\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n",
        "\n",
        "# final_time = (time.time()- start_time)/60\n",
        "# print(f'The time in minutes: {final_time}')"
      ],
      "metadata": {
        "id": "0f9r_P9wkUY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.head()"
      ],
      "metadata": {
        "id": "B8JY56kXkjWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss[['accuracy','val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "yCr_tQ-ukqe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(valid_padded)\n",
        "predictions"
      ],
      "metadata": {
        "id": "iNSjafc2k2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_sequences2 = tokenizer.texts_to_sequences(test_df[\"Title Campaign\"])\n",
        "testing_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "yb83wAksMStm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df['Title Campaign'][0])\n",
        "print(testing_sequences2[0])\n",
        "print(testing_padded2[0])"
      ],
      "metadata": {
        "id": "WDISEBnzMZSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testing_padded2)\n",
        "predictions"
      ],
      "metadata": {
        "id": "wHfe32VjlYSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saved Model in .H5 Files#"
      ],
      "metadata": {
        "id": "boy1PUA8XsxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/my_model.h5')"
      ],
      "metadata": {
        "id": "IcyTX24VOZz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('/my_model.h5')\n",
        "new_model.summary()"
      ],
      "metadata": {
        "id": "OTLJVcUsXept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Bersama Menjaga Lingkungan Asri']\n",
        "text_test = pd.DataFrame(text, columns=['Campaign'])\n",
        "\n",
        "text_clean = text_test['Campaign'].apply(lambda x : clean_text(x))\n",
        "text_final = text_test['Campaign'].apply(lambda x : steaming(x))\n",
        "\n",
        "print(text_final)"
      ],
      "metadata": {
        "id": "r-DCY-1kYNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text_sequence = tokenizer.texts_to_sequences(text_final)\n",
        "testing_text_padded = pad_sequences(testing_text_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(testing_text_sequence)\n",
        "print(testing_text_padded)"
      ],
      "metadata": {
        "id": "vw7MJENJZlCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = new_model.predict(testing_text_padded)\n",
        "predictions"
      ],
      "metadata": {
        "id": "q5L83pY6Xrau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saved Model TFJS#"
      ],
      "metadata": {
        "id": "dmzvod0pX61b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
        "\n",
        "# model.save(saved_model_path)"
      ],
      "metadata": {
        "id": "lI5wOsSSp8Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# !pip install tensorflowjs\n",
        "\n",
        "# !tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ],
      "metadata": {
        "id": "qd7ThH_or_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3u8AqyIsQue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}