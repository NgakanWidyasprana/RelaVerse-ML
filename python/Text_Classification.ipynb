{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dmzvod0pX61b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare Model and Convert**"
      ],
      "metadata": {
        "id": "aNIMCqLBBLTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeXATNXBBPVp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "# import time\n",
        "# from wordcloud import WordCloud, STOPWORDS\n",
        "# from IPython.core.display import display, HTML\n",
        "# import plotly.graph_objects as go\n",
        "# Natural Language Tool Kit\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# import cufflinks as cf\n",
        "# cf.go_offline()\n",
        "## Make numpy values easier to read.\n",
        "# np.set_printoptions(precision=3, suppress=True)"
      ],
      "metadata": {
        "id": "5FrjS8UIWlGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/Train.csv\")\n",
        "test_df = pd.read_csv(\"https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/Test.csv\")"
      ],
      "metadata": {
        "id": "7hUHzxt9BZpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "id": "3uhziBX3AyZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "id": "gqIJu-bEBLUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "\n",
        "id_stopword_dict = pd.read_csv('https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/stopwordbahasa.csv', header=None)\n",
        "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})"
      ],
      "metadata": {
        "id": "zAGpkhUPadhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(id_stopword_dict)"
      ],
      "metadata": {
        "id": "JwYGJo06BhNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopword(text):\n",
        "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "LLtK_YNsc8ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    # split to array (default delimiter is \" \")\n",
        "    text = remove_stopword(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "aEhyO2vXXzlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : clean_text(x))\n",
        "test_df['Title Campaign'] = test_df['Title Campaign'].apply(lambda x : clean_text(x))"
      ],
      "metadata": {
        "id": "Qa2Ip9gpdZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "9pJnryuvd1jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "qzWhbdQkeG0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySastrawi"
      ],
      "metadata": {
        "id": "l2PIM787eQ9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steaming\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def steaming(text):\n",
        "  return stemmer.stem(text)"
      ],
      "metadata": {
        "id": "1PX9_yFBewYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer)"
      ],
      "metadata": {
        "id": "sTsdiL8RCq9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : steaming(x))\n",
        "test_df['Title Campaign'] = test_df['Title Campaign'].apply(lambda x : steaming(x))"
      ],
      "metadata": {
        "id": "HZi9KsWKCras"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "neMeg5xCfPw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "bZ_5_IOcfVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many unique words have this text\n",
        "\n",
        "def counter_word(text):\n",
        "    count = Counter()\n",
        "    for i in text.values:\n",
        "        for word in i.split():\n",
        "            count[word] += 1\n",
        "    return count\n",
        "\n",
        "text_values = train_df['Title Campaign']\n",
        "\n",
        "counter = counter_word(text_values)\n",
        "print(f\"The len of words is: {len(counter)}\")\n",
        "list(counter.items())[:10]"
      ],
      "metadata": {
        "id": "tRJb-GkIfay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "\n",
        "vocab_size = len(counter)\n",
        "embedding_dim = 32\n",
        "\n",
        "# Max number of words in each complaint\n",
        "max_length = 20\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "\n",
        "# oov_took its set for words out our word index\n",
        "oov_tok = \"<XXX>\"\n",
        "training_size = 80\n",
        "seq_len = 12\n",
        "\n",
        "# based on 80% of the data\n",
        "training_sentences = train_df['Title Campaign'][0:training_size]\n",
        "training_labels = train_df.Label[0:training_size]\n",
        "\n",
        "valid_sentences = train_df['Title Campaign'][training_size:]\n",
        "valid_labels = train_df.Label[training_size:]\n",
        "\n",
        "print('The Shape of training ',training_sentences.shape)\n",
        "print('The Shape of validation',valid_sentences.shape)"
      ],
      "metadata": {
        "id": "tP1GSGeYfs_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "e9MqP4QgiMFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "id": "ExZy8u2sDdPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the first 10 elements\n",
        "print(\"THe first word Index are: \")\n",
        "for x in list(word_index)[0:15]:\n",
        "    print (\" {},  {} \".format(x,  word_index[x]))"
      ],
      "metadata": {
        "id": "ocixmswhioHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "XNfCESC-iq_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence : {}\".format(train_df['Title Campaign'][0]))\n",
        "print(\"Text Sequences : {}\".format(training_sequences[0]))\n",
        "print(\"Text Padded : {}\".format(training_padded[0]))"
      ],
      "metadata": {
        "id": "Z0uBG_GgET3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
        "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "CA4_wyBgj566"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence : \\n{}\".format(pd.Series(data=valid_sentences, index = [80])))\n",
        "print(\"\\nText Sequences : {}\".format(valid_sequences[0]))\n",
        "print(\"Text Padded : {}\".format(valid_padded[0]))"
      ],
      "metadata": {
        "id": "G2ExJ9ByEGJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # remember this is a binary classification\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H6hfACKEkIAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_time = time.time()\n",
        "\n",
        "num_epochs = 40\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n",
        "\n",
        "# final_time = (time.time()- start_time)/60\n",
        "# print(f'The time in minutes: {final_time}')"
      ],
      "metadata": {
        "id": "0f9r_P9wkUY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.head()"
      ],
      "metadata": {
        "id": "B8JY56kXkjWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss[['accuracy','val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "yCr_tQ-ukqe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(valid_padded)\n",
        "predictions"
      ],
      "metadata": {
        "id": "iNSjafc2k2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_sequences2 = tokenizer.texts_to_sequences(test_df[\"Title Campaign\"])\n",
        "testing_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "yb83wAksMStm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df['Title Campaign'][0])\n",
        "print(testing_sequences2[0])\n",
        "print(testing_padded2[0])"
      ],
      "metadata": {
        "id": "WDISEBnzMZSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testing_padded2)\n",
        "predictions"
      ],
      "metadata": {
        "id": "wHfe32VjlYSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saved Model in .H5 Files**"
      ],
      "metadata": {
        "id": "boy1PUA8XsxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section will save model into .h5 files, so you can use this for converting to TFJS or TF Lite. Also with this format you can use directly in python without repeat compile a model."
      ],
      "metadata": {
        "id": "mhgOnysDFspK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/my_model.h5')"
      ],
      "metadata": {
        "id": "IcyTX24VOZz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Used Saved Model For Classification**"
      ],
      "metadata": {
        "id": "vmTMt_2LGWzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before use model the step by step that mist follow is like this :\n",
        "\n",
        "\n",
        "---\n",
        "1.   Preprocessing Data\n",
        "2.   Load Model\n",
        "3.   Tokenization and Padding\n",
        "4.   Use Model to Predict Result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "imOQ4YQFHyb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Preprocessing Data and Prediction with Model**"
      ],
      "metadata": {
        "id": "leanBR1HHpXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 1 : Preparing Model**\n",
        "\n",
        "You need add model machine learning .h5 from local to google colab first before do next step. After that you can check the model architecture with **this** code, if you have add model"
      ],
      "metadata": {
        "id": "DZnbeRFl87VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('/my_model.h5')\n",
        "new_model.summary()"
      ],
      "metadata": {
        "id": "OTLJVcUsXept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 2 : Preparing Vocab and Function Preprocessing**\n",
        "\n",
        "This phase you will prepare the vocabulary using trainning dataset because that is you use for the trainning in model before. But you also need to pre-processing the data first so make more accurate and don't have incosisten meaning"
      ],
      "metadata": {
        "id": "Z9D85gS_9McI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Based Vocab, Stopword, and Stemming\n",
        "train_df = pd.read_csv(\"https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/Train.csv\")\n",
        "id_stopword_dict = pd.read_csv('https://raw.githubusercontent.com/NgakanWidyasprana/title-campaign-classification/main/Dataset/stopwordbahasa.csv', header=None)\n",
        "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Function pre-processing\n",
        "def remove_stopword(text):\n",
        "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    # split to array (default delimiter is \" \")\n",
        "    text = remove_stopword(text)\n",
        "    return text\n",
        "\n",
        "def steaming(text):\n",
        "  return stemmer.stem(text)\n",
        "\n",
        "# Preprocessing Data\n",
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : clean_text(x))\n",
        "train_df['Title Campaign'] = train_df['Title Campaign'].apply(lambda x : steaming(x))\n",
        "\n",
        "# Make Vocabulary\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "vocab_size = 285\n",
        "embedding_dim = 32\n",
        "# Max number of words in each complaint\n",
        "max_length = 20\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "# oov_took its set for words out our word index\n",
        "oov_tok = \"<XXX>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "zEeTXG4960vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3 : Preparing Prediction for Model**\n",
        "\n",
        "This phase you will prepare the prediction for title that you want to predict with the model. Before to use it, you must pre-processing the input first and predict in the last"
      ],
      "metadata": {
        "id": "kHwfF7xK_7xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Bersama Menjaga Lingkungan Asri']\n",
        "text_test = pd.DataFrame(text, columns=['Campaign'])\n",
        "\n",
        "text_clean = text_test['Campaign'].apply(lambda x : clean_text(x))\n",
        "text_final = text_test['Campaign'].apply(lambda x : steaming(x))\n",
        "\n",
        "print(text_final)"
      ],
      "metadata": {
        "id": "r-DCY-1kYNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_text_sequence = tokenizer.texts_to_sequences(text_final)\n",
        "testing_text_padded = pad_sequences(testing_text_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(testing_text_sequence)\n",
        "print(testing_text_padded)"
      ],
      "metadata": {
        "id": "vw7MJENJZlCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = new_model.predict(testing_text_padded)\n",
        "predictions"
      ],
      "metadata": {
        "id": "q5L83pY6Xrau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saved Model TFJS#"
      ],
      "metadata": {
        "id": "dmzvod0pX61b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
        "\n",
        "# model.save(saved_model_path)"
      ],
      "metadata": {
        "id": "lI5wOsSSp8Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# !pip install tensorflowjs\n",
        "\n",
        "# !tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ],
      "metadata": {
        "id": "qd7ThH_or_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3u8AqyIsQue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}